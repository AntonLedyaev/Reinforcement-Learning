# Reinforcement-Learning

## Задача стабилизации подвижного маятника на тележке.

Рассматривается задача стабилизации перевернутого маятника, расположенного на подвижной платформе. Маятник удерживается в перевернутом состоянии за счет изменения скорости тележки.

![Image alt](https://github.com/AntonLedyaev/Reinforcement-Learning/raw/main/img/cartpole.gif)


## Описание среды.

Состояние тележки описывается следующими параметрами:

* позиция тележки – значение в диапазоне [-4.8, 4.8];
* скорость тележки;
* угол отклонения шеста от вертикали – значение в диапазоне [-24°, 24°];
* скорость изменения угла наклона шеста.

Действие принимает два значения - 0 и 1:

* 0 – толкнуть тележку влево (приложить к тележке горизонтальную силу, равную +1);
* 1 – толкнуть тележку вправо (приложить к тележке горизонтальную силу, равную -1).

Эпизод завершается если:

* угол шеста вышел из диапазона [-24°, 24°];
* позиция тележки вышла из допустимого диапазона [-4.8, 4.8];
* длина эпизода превышает 500;


## Q-learning
Q-learning это не связанный с политикой без модельный алгоритм ОП, основанный на хорошо известном уравнении Беллмана:

![Image alt](https://github.com/AntonLedyaev/Reinforcement-Learning/raw/main/img/bellman1.png)

Мы можем переписать это уравнение в форме Q-value:

![Image alt](https://github.com/AntonLedyaev/Reinforcement-Learning/raw/main/img/bellman 2.png)

Оптимальное значение Q, обозначенное как Q*, может быть выражено как:

![Image alt](https://github.com/AntonLedyaev/Reinforcement-Learning/raw/main/img/bellman.png)

Цель состоит в том, чтобы максимизировать Q-значение.


